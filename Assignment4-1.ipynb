{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DSC 478__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Jonathan Lynch__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: Load in the joke ratings data and the joke text data into appropriate data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlynch/Desktop/DSC 478/jokes\n"
     ]
    }
   ],
   "source": [
    "%cd \"/Users/jonathanlynch/Desktop/DSC 478/jokes\"\n",
    "\n",
    "ratings = np.genfromtxt('modified_jester_data.csv', delimiter=',')\n",
    "\n",
    "jokes = np.genfromtxt('jokes.csv', delimiter=',',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer\\'s disease\". The man replies \"Well thank God I don\\'t have cancer!\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes[0][1]  # look at first joke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B: Complete the definition for the function \"test\".  Use this function to perform evaluation (with 20% test-ratio for each user) comparing MAE results using standard item-based collaborative filtering (based on the rating prediction function \"standEst\") with results using the SVD-based version of the rating item-based CF (using \"svdEst\" as the prediction engine):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code provided in itemBasedRec.py (from Chap 14 of MLA):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecludSim(inA,inB):\n",
    "    return 1.0 / (1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsSim(inA,inB):\n",
    "    if len(inA) < 3 : return 1.0\n",
    "    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosSim(inA,inB):\n",
    "    num = float(inA.T * inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5 + 0.5 * (num / denom)\n",
    "\n",
    "def standEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    for j in range(n):\n",
    "        userRating = dataMat[user,j]\n",
    "        if userRating == 0: continue\n",
    "        overLap = nonzero(logical_and(dataMat[:,item]>0, \\\n",
    "                                      dataMat[:,j]>0))[0]\n",
    "        if len(overLap) == 0: similarity = 0\n",
    "        else: similarity = simMeas(dataMat[overLap,item], \\\n",
    "                                   dataMat[overLap,j])\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "\n",
    "def svdEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(dataMat)\n",
    "    U,Sigma,VT = la.svd(data)\n",
    "    Sig4 = mat(eye(4)*Sigma[:4]) #arrange Sig4 into a diagonal matrix\n",
    "    xformedItems = data.T * U[:,:4] * Sig4.I  #create transformed items\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        similarity = simMeas(xformedItems[item,:].T,\\\n",
    "                             xformedItems[j,:].T)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "\n",
    "def cross_validate_user(dataMat, user, test_ratio, estMethod, simMeas):\n",
    "    number_of_items = np.shape(dataMat)[1]\n",
    "    rated_items_by_user = np.array([i for i in range(number_of_items) if dataMat[user,i]>0])\n",
    "    test_size = int(test_ratio * len(rated_items_by_user))\n",
    "    test_indices = np.random.randint(0, len(rated_items_by_user), test_size)\n",
    "    withheld_items = rated_items_by_user[test_indices]\n",
    "    original_user_profile = np.copy(dataMat[user])\n",
    "    dataMat[user, withheld_items] = 0 # so that the withheld test items is not used in the rating estimation below\n",
    "    error_u = 0.0\n",
    "    count_u = len(withheld_items)\n",
    "\n",
    "    # compute absolute error for user u over all test items\n",
    "    for item in withheld_items:\n",
    "        # estimate rating on the withheld item\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        error_u = error_u + abs(estimatedScore - original_user_profile[item])\n",
    "\n",
    "    # restore ratings of the withheld items to the user profile\n",
    "    for item in withheld_items:\n",
    "        dataMat[user, item] = original_user_profile[item]\n",
    "\n",
    "    # return sum of absolute errors and the count of test cases for this user\n",
    "    return error_u, count_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Complete function \"test\" definition:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataMat, test_ratio, estMethod, simMeas):\n",
    "    'function to iterate over all users, perform cross-validation, and print MAE'\n",
    "    total_error = 0\n",
    "    total_count = 0\n",
    "    for user in range(dataMat.shape[0]):\n",
    "        error_u, count_u = cross_validate_user(dataMat, user, test_ratio, estMethod, simMeas)\n",
    "        total_error += error_u\n",
    "        total_count += count_u\n",
    "    MAE = total_error/total_count\n",
    "    print ('Mean Absoloute Error for ',estMethod,' : ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compare MAE results using standard item-based collaborative filtering vs SVD-based version:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for  <function standEst at 0x7ff57f873c10>  :  3.692740366748648\n"
     ]
    }
   ],
   "source": [
    "test(ratings, .2, standEst, pearsSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error using standard item-based collaborative filtering (based on the rating prediction function \"standEst\") and a Pearson similarity measure is approximately 3.69."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for  <function svdEst at 0x7ff57f873ca0>  :  3.636041315959524\n"
     ]
    }
   ],
   "source": [
    "test(ratings, .2, svdEst, pearsSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error using the SVD-based version of the rating item-based collaborative filtering (based on the function \"svdEst\") and a Pearson similarity measure is approximately 3.64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Write a new function \"print_most_similar_jokes\" which takes the joke ratings data, a query joke id, a parameter k for the number of nearest neighbors, and a similarity metric function, and prints the text of the query joke as well as the texts of the top k most similar jokes based on user ratings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write function \"print_most_similar_jokes\" to display the k most similar jokes, given a query joke & a similarity metric function:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_jokes(data, jokes, joke, k, metric):\n",
    "    'function to print k most similar jokes using selected similarity metric function'\n",
    "    print (\"Selcted joke: \")\n",
    "    print()\n",
    "    print (jokes[joke][1])                          # print selected joke\n",
    "    print()\n",
    "    distances = []\n",
    "    joke_vector = data[:,joke]                      # vector of joke ratings for selected joke\n",
    "    for i in range(data.shape[1]):\n",
    "        joke_vectors = data[:,i]                    # vectors of joke ratings for each joke\n",
    "        dist = metric(joke_vector, joke_vectors)    # distances based on similarity measure\n",
    "        distances.append(dist)\n",
    "    distances_array = np.array(distances)\n",
    "    sorted_distances = distances_array.argsort()  # get sorted indicies of distances_array\n",
    "    top_indicies = sorted_distances[:k]           # top k indicies\n",
    "    print(\"The top %d recommended jokes are: \"%(k))\n",
    "    for indice in top_indicies:\n",
    "        top_joke = jokes[indice]       # top jokes\n",
    "        print()\n",
    "        print(top_joke[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Look at 5 most similar jokes to joke #3, using a Euclidean distance metric function:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selcted joke: \n",
      "\n",
      "Q. What's the difference between a man and a toilet? A. A toilet doesn't follow you around after you use it.\n",
      "\n",
      "The top 5 recommended jokes are: \n",
      "\n",
      "A guy goes into confession and says to the priest \"Father I'm 80 years old widower with 11 grandchildren. Last night I met two beautiful flight attendants. They took me home and I made love to both of them. Twice.\"The priest said: \"Well my son when was the last time you were in confession?\" \"Never Father I'm Jewish.\" \"So then why are you telling me?\" \"I'm telling everybody.\"\n",
      "\n",
      "A guy walks into a bar orders a beer and says to the bartender\"Hey I got this great Polish Joke...\" The barkeep glares at him and says in a warning tone of voice:\"Before you go telling that joke you better know that I'm Polish both bouncers are Polish and so are most of my customers\"\"Okay\" says the customer\"I'll tell it very slowly.\"\n",
      "\n",
      "A group of  managers were given the assignment to measure the height of a flagpole. So they go out to the flagpole with ladders and tape measures and they're falling off the ladders dropping the tape measures - the whole thing is just a mess.An engineer comes along and sees what they're trying to do walks over pulls the flagpole out of the ground lays it flat measures it from end to end gives the measurement to one of the managers and walks away.After the engineer has gone one manager turns to another and laughs. \"Isn't that just like an engineer we're looking for the height and he gives us the length.\"\n",
      "\n",
      "An explorer in the deepest Amazon suddenly finds himself surrounded by a bloodthirsty group of natives. Upon surveying the situation he says quietly to himself \"Oh God I'm screwed.\" The sky darkens and a voice booms out \"No you are NOTscrewed. Pick up that stone at your feet and bash in the head of the chief standing in front of you.\" So with the stone he bashes the life out of the chief. Standing above the lifeless body breathing heavily looking at 100 angry natives... The voice booms out again \"Okay ..... NOW you're screwed.\"\n",
      "\n",
      "An old Scotsmen is sitting with a younger Scottish gentleman and says the boy. \"Ah lad look out that window. You see that stone wall there I built it with me own bare hands placed every stone meself.  But do they call me MacGregor the wall builder? No! He Takes a few sips of his beer then says \"Aye and look out on that lake and eye that beautiful pier. I built it meself laid every board and hammered each nail but do they call me MacGregor the pier builder? No! He continues...\"And lad you see that road? That too I build with me own bare hands. Laid every inch of pavement meself but do they call MacGregor the roadbuilder? No!\"Again he returns to his beer for a few sips then says \"Agh but you screw one sheep...\"\n"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(ratings, jokes, 3, 5, ecludSim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
